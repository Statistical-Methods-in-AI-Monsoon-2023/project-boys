{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../combined_reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.load('./new_X.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a NumPy array\n",
    "df_array = df.to_numpy()\n",
    "\n",
    "# Stack the DataFrame and the NumPy array horizontally\n",
    "X_final = np.hstack((df_array, X1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.read_csv('../kag_red.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X = combined_df.drop(['subreddit', 'post'],  axis=1)  # Features\n",
    "y = combined_df['subreddit']  # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample this data for better results\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_final, y = ros.fit_resample(X_final, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddit\n",
      "adhd             117331\n",
      "autism           117331\n",
      "ptsd             117331\n",
      "suicidewatch     117331\n",
      "lonely           117331\n",
      "alcoholism       117331\n",
      "schizophrenia    117331\n",
      "depression       117331\n",
      "addiction        117331\n",
      "anxiety          117331\n",
      "bpd              117331\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print count of each class\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X_final  , y, test_size=0.2, random_state=42)\n",
    "# split into train, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "\n",
    "# Step 1: Encode the categorical target variable (y_train) using OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_encoded = encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_val_encoded = encoder.transform(y_val.values.reshape(-1, 1))\n",
    "\n",
    "# Step 2: Flatten the encoded labels\n",
    "y_train_encoded = y_train_encoded.argmax(axis=1)\n",
    "y_val_encoded = y_val_encoded.argmax(axis=1)\n",
    "y_test_encoded = encoder.transform(y_test.values.reshape(-1, 1)).argmax(axis=1)\n",
    "y_train_encoded = to_categorical(y_train_encoded)  # Convert to one-hot encoding\n",
    "y_val_encoded = to_categorical(y_val_encoded)  # Convert to one-hot encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define the CNN model\n",
    "# model = Sequential()\n",
    "# model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dense(y_train_encoded.shape[1], activation='softmax'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Reshape X_train to fit the Conv1D input shape\n",
    "# X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "# # Train the model\n",
    "# model.fit(X_train_reshaped, y_train_encoded, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# # Evaluate on the test set\n",
    "# X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "# y_pred = model.predict(X_test_reshaped)\n",
    "# y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "# y_test_encoded = encoder.transform(y_test.values.reshape(-1, 1)).argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Evaluate the model\n",
    "# accuracy = accuracy_score(y_test_encoded, y_pred_classes)\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(y_test_encoded, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming you already have predictions in y_pred_classes and the true labels in y_test_encoded\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# cm = confusion_matrix(y_test_encoded, y_pred_classes)\n",
    "\n",
    "# # Display the confusion matrix using seaborn heatmap\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.categories_[0], yticklabels=encoder.categories_[0])\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.xlabel('Predicted Labels')\n",
    "# plt.ylabel('True Labels')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'CNN_combined.ipynb'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import wandb\n",
    "# # from wandb.keras import WandbCallback\n",
    "# # from tensorflow.keras.models import Sequential\n",
    "# # from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "# # from tensorflow.keras.callbacks import EarlyStopping\n",
    "# # from sklearn.metrics import accuracy_score, classification_report\n",
    "# # import numpy as np\n",
    "\n",
    "# # wandb.init(project='Reddit_mental')\n",
    "\n",
    "\n",
    "# # def create_model():\n",
    "# #     model = Sequential()\n",
    "# #     model.add(Conv1D(filters=wandb.config.filters, kernel_size=wandb.config.kernel_size, activation=wandb.config.act, input_shape=(X_train.shape[1], 1)))\n",
    "# #     model.add(MaxPooling1D(pool_size=wandb.config.pool_size))\n",
    "# #     model.add(Flatten())\n",
    "# #     model.add(Dense(wandb.config.dense_units, activation=wandb.config.act))\n",
    "# #     model.add(Dense(y_train_encoded.shape[1], activation='softmax'))\n",
    "\n",
    "# #     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# #     return model\n",
    "\n",
    "\n",
    "# # sweep_config = {\n",
    "# #     'method': 'random',\n",
    "# #     'metric': {'name': 'val_loss', 'goal': 'minimize'},\n",
    "# #     'parameters': {\n",
    "# #         'filters': {'values': [16, 32, 64]},\n",
    "# #         'kernel_size': {'values': [3, 5]},\n",
    "# #         'pool_size': {'values': [2, 3]},\n",
    "# #         'dense_units': {'values': [50, 100]},\n",
    "# #         'act': {'values': ['relu', 'tanh']}\n",
    "# #     }\n",
    "# # }\n",
    "\n",
    "# # sweep_id = wandb.sweep(sweep_config, project='reddit_mental')\n",
    "\n",
    "# # def train():\n",
    "# #     print('start')\n",
    "# #     with wandb.init() as run:\n",
    "# #         print(1)\n",
    "# #         config = wandb.config\n",
    "# #         print(2)\n",
    "# #         model = create_model()\n",
    "# #         print(3)\n",
    "# #         X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "# #         print(4)\n",
    "# #         X_val_reshaped = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "# #         print(5)\n",
    "\n",
    "# #         # Train the model\n",
    "# #         history = model.fit(X_train_reshaped, y_train_encoded, epochs=10, batch_size=64,\n",
    "# #                             validation_data=(X_val_reshaped, y_val_encoded),\n",
    "# #                             callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)])\n",
    "# #         print(6)\n",
    "# #         # Evaluation\n",
    "# #         X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "# #         y_pred = model.predict(X_test_reshaped)\n",
    "# #         y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "# #         accuracy = accuracy_score(y_test_encoded, y_pred_classes)\n",
    "# #         wandb.log({'test_accuracy': accuracy})\n",
    "\n",
    "# #         # Log plots\n",
    "# #         # (You can log plots like training/validation loss, accuracy, etc. from the 'history' object)\n",
    "# #         # wandb.log({'training_loss': history.history['loss'][9], 'epoch': 10})\n",
    "# #         wandb.log({'validation_loss': history.history['val_loss'][9], 'epoch': 10})\n",
    "# #         print('end')\n",
    "\n",
    "# import wandb\n",
    "# from wandb.keras import WandbCallback\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import numpy as np\n",
    "\n",
    "# wandb.init(project='Reddit_mental')\n",
    "\n",
    "# # Your data: X_train, X_val, X_test, y_train_encoded, y_val_encoded, y_test_encoded\n",
    "\n",
    "# # Manually defined parameters\n",
    "# parameters = [\n",
    "#     {'filters': 16, 'kernel_size': 3, 'pool_size': 2, 'dense_units': 50, 'act': 'relu'},\n",
    "#     {'filters': 16, 'kernel_size': 3, 'pool_size': 2, 'dense_units': 50, 'act': 'tanh'},\n",
    "#     {'filters': 16, 'kernel_size': 3, 'pool_size': 2, 'dense_units': 100, 'act': 'relu'},\n",
    "#     {'filters': 16, 'kernel_size': 5, 'pool_size': 3, 'dense_units': 50, 'act': 'tanh'},\n",
    "#     {'filters': 16, 'kernel_size': 5, 'pool_size': 2, 'dense_units': 100, 'act': 'tanh'},\n",
    "#     {'filters': 16, 'kernel_size': 7, 'pool_size': 3, 'dense_units': 50, 'act': 'relu'},\n",
    "#     {'filters': 32, 'kernel_size': 3, 'pool_size': 2, 'dense_units': 100, 'act': 'relu'},\n",
    "#     {'filters': 16, 'kernel_size': 5, 'pool_size': 2, 'dense_units': 50, 'act': 'tanh'},\n",
    "#     {'filters': 16, 'kernel_size': 7, 'pool_size': 2, 'dense_units': 50, 'act': 'relu'},\n",
    "#     {'filters': 32, 'kernel_size': 5, 'pool_size': 3, 'dense_units': 100, 'act': 'relu'},\n",
    "#     {'filters': 32, 'kernel_size': 7, 'pool_size': 3, 'dense_units': 100, 'act': 'tanh'},\n",
    "#     {'filters': 32, 'kernel_size': 5, 'pool_size': 3, 'dense_units': 50, 'act': 'tanh'},\n",
    "#     {'filters': 32, 'kernel_size': 7, 'pool_size': 3, 'dense_units': 50, 'act': 'relu'},\n",
    "#     {'filters': 32, 'kernel_size': 3, 'pool_size': 2, 'dense_units': 50, 'act': 'relu'},\n",
    "#     {'filters': 16, 'kernel_size': 5, 'pool_size': 2, 'dense_units': 120, 'act': 'tanh'},\n",
    "#     {'filters': 16, 'kernel_size': 3, 'pool_size': 2, 'dense_units': 100, 'act': 'tanh'},\n",
    "\n",
    "# ]\n",
    "\n",
    "# def create_model(filters, kernel_size, pool_size, dense_units, act):\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=act, input_shape=(X_train.shape[1], 1)))\n",
    "#     model.add(MaxPooling1D(pool_size=pool_size))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(dense_units, activation=act))\n",
    "#     model.add(Dense(y_train_encoded.shape[1], activation='softmax'))\n",
    "\n",
    "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     return model\n",
    "\n",
    "# for i, params in enumerate(parameters):\n",
    "#     print(f\"Training iteration {i+1} with parameters: {params}\")\n",
    "#     model = create_model(**params)\n",
    "#     X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "#     X_val_reshaped = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "\n",
    "#     history = model.fit(X_train_reshaped, y_train_encoded, epochs=10, batch_size=64,\n",
    "#                         validation_data=(X_val_reshaped, y_val_encoded),\n",
    "#                         callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "#                                    WandbCallback()])\n",
    "    \n",
    "#     X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "#     y_pred = model.predict(X_test_reshaped)\n",
    "#     y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "#     accuracy = accuracy_score(y_test_encoded, y_pred_classes)\n",
    "    \n",
    "#     wandb.log({'test_accuracy': accuracy})\n",
    "#     wandb.log({'validation_loss': history.history['val_loss'][len(history.history['val_loss']) - 1], 'epoch': len(history.history['val_loss'])})\n",
    "\n",
    "#     print(f\"Iteration {i+1} completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maayushach16\u001b[0m (\u001b[33mboys69\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/boys69/Reddit_mental/runs/wmsprz2e' target=\"_blank\">ancient-galaxy-29</a></strong> to <a href='https://wandb.ai/boys69/Reddit_mental' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/boys69/Reddit_mental' target=\"_blank\">https://wandb.ai/boys69/Reddit_mental</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/boys69/Reddit_mental/runs/wmsprz2e' target=\"_blank\">https://wandb.ai/boys69/Reddit_mental/runs/wmsprz2e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "wandb.init(project='Reddit_mental')\n",
    "\n",
    "# Your data: X_train, X_val, X_test, y_train_encoded, y_val_encoded, y_test_encoded\n",
    "\n",
    "# Manual specification of parameters for the first run\n",
    "params = {\n",
    "    'filters': 16, 'kernel_size': 3, 'pool_size': 2, 'dense_units': 200, 'act': 'tanh'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(774384, 1141)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12094/12100 [============================>.] - ETA: 0s - loss: 0.7265 - accuracy: 0.7605"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yeetusonthefetus/iiit/SMAI/project-boys/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12100/12100 [==============================] - 124s 10ms/step - loss: 0.7265 - accuracy: 0.7606 - val_loss: 0.5795 - val_accuracy: 0.8075\n",
      "Epoch 2/10\n",
      "10492/12100 [=========================>....] - ETA: 16s - loss: 0.4953 - accuracy: 0.8336"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12096/12100 [============================>.] - ETA: 0s - loss: 0.4886 - accuracy: 0.8358"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yeetusonthefetus/iiit/SMAI/project-boys/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12100/12100 [==============================] - 135s 11ms/step - loss: 0.4886 - accuracy: 0.8358 - val_loss: 0.4468 - val_accuracy: 0.8496\n",
      "Epoch 3/10\n",
      "12100/12100 [==============================] - ETA: 0s - loss: 0.3961 - accuracy: 0.8644"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yeetusonthefetus/iiit/SMAI/project-boys/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12100/12100 [==============================] - 132s 11ms/step - loss: 0.3961 - accuracy: 0.8644 - val_loss: 0.4176 - val_accuracy: 0.8596\n",
      "Epoch 4/10\n",
      "12099/12100 [============================>.] - ETA: 0s - loss: 0.3507 - accuracy: 0.8784"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yeetusonthefetus/iiit/SMAI/project-boys/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12100/12100 [==============================] - 148s 12ms/step - loss: 0.3507 - accuracy: 0.8784 - val_loss: 0.3753 - val_accuracy: 0.8720\n",
      "Epoch 5/10\n",
      "12098/12100 [============================>.] - ETA: 0s - loss: 0.3219 - accuracy: 0.8871"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yeetusonthefetus/iiit/SMAI/project-boys/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12100/12100 [==============================] - 160s 13ms/step - loss: 0.3219 - accuracy: 0.8871 - val_loss: 0.3700 - val_accuracy: 0.8740\n",
      "Epoch 6/10\n",
      "12096/12100 [============================>.] - ETA: 0s - loss: 0.3013 - accuracy: 0.8935"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yeetusonthefetus/iiit/SMAI/project-boys/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12100/12100 [==============================] - 160s 13ms/step - loss: 0.3013 - accuracy: 0.8935 - val_loss: 0.3617 - val_accuracy: 0.8765\n",
      "Epoch 7/10\n",
      "12097/12100 [============================>.] - ETA: 0s - loss: 0.2841 - accuracy: 0.8991"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yeetusonthefetus/iiit/SMAI/project-boys/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12100/12100 [==============================] - 140s 12ms/step - loss: 0.2841 - accuracy: 0.8991 - val_loss: 0.3521 - val_accuracy: 0.8823\n",
      "Epoch 8/10\n",
      "12097/12100 [============================>.] - ETA: 0s - loss: 0.2708 - accuracy: 0.9030"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yeetusonthefetus/iiit/SMAI/project-boys/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12100/12100 [==============================] - 143s 12ms/step - loss: 0.2708 - accuracy: 0.9030 - val_loss: 0.3440 - val_accuracy: 0.8840\n",
      "Epoch 9/10\n",
      "12100/12100 [==============================] - ETA: 0s - loss: 0.2576 - accuracy: 0.9077"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yeetusonthefetus/iiit/SMAI/project-boys/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/wandb/run-20231116_180921-wmsprz2e/files/model-best)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12100/12100 [==============================] - 163s 13ms/step - loss: 0.2576 - accuracy: 0.9077 - val_loss: 0.3408 - val_accuracy: 0.8877\n",
      "Epoch 10/10\n",
      "12100/12100 [==============================] - 160s 13ms/step - loss: 0.2467 - accuracy: 0.9112 - val_loss: 0.3450 - val_accuracy: 0.8872\n",
      "8067/8067 [==============================] - 15s 2ms/step\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=params['filters'], kernel_size=params['kernel_size'], activation=params['act'],\n",
    "                 input_shape=(X_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=params['pool_size']))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(params['dense_units'], activation=params['act']))\n",
    "model.add(Dense(y_train_encoded.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_val_reshaped = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "\n",
    "history = model.fit(X_train_reshaped, y_train_encoded, epochs=10, batch_size=64,\n",
    "                    validation_data=(X_val_reshaped, y_val_encoded),\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "                               WandbCallback()])\n",
    "\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred_classes)\n",
    "\n",
    "wandb.log({'test_accuracy': accuracy})\n",
    "wandb.log({'validation_loss': history.history['val_loss'][len(history.history['val_loss']) - 1], 'epoch': len(history.history['val_loss'])})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
