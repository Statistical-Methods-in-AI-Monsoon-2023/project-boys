{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/yeetusonthefetus/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yeetusonthefetus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas \n",
    "import os\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load the data from combined_df.csv\n",
    "combined_df = pandas.read_csv('../combined_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and clean text data (you can use regular expressions for more advanced cleaning)\n",
    "combined_df['post'] = combined_df['post'].apply(lambda x: x.lower())  # Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yeetusonthefetus/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yeetusonthefetus/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yeetusonthefetus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenization and removing punctuation\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "    \n",
    "    # Removing stop words and lemmatization\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [WordNetLemmatizer().lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return \" \".join(words)\n",
    "    \n",
    "combined_df['cleaned_posts'] = combined_df['post'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "\n",
    "documents = combined_df['cleaned_posts'].str.split()  # Split the preprocessed text into words\n",
    "\n",
    "# Create a dictionary mapping words to IDs\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# Create a bag of words corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)  # Adjust num_topics as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distributions = [lda_model[doc] for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = max(len(topic) for topic in topic_distributions)\n",
    "topic_features = np.zeros((len(topic_distributions), num_topics))  # num_topics is the number of topics in your LDA model\n",
    "\n",
    "for i, doc_topics in enumerate(topic_distributions):\n",
    "    for topic, weight in doc_topics:\n",
    "        topic_features[i, topic] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_topic = topic_features.shape  # Shape of the topic modeling output\n",
    "\n",
    "# Create an empty array to accommodate the topic modeling data\n",
    "X_combined = np.empty((shape_topic[0], shape_topic[1]))\n",
    "\n",
    "# Copy data from the original topic modeling array to the combined array\n",
    "X_combined[:, :shape_topic[1]] = topic_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = combined_df.drop(['subreddit', 'post'],  axis=1)  # Features\n",
    "y = combined_df['subreddit']  # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined  , y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299599, 10)\n",
      "(299599,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "14980/14980 [==============================] - 543s 36ms/step - loss: 1.9763 - accuracy: 0.3129 - val_loss: 1.9771 - val_accuracy: 0.3144\n",
      "Epoch 2/10\n",
      "14980/14980 [==============================] - 541s 36ms/step - loss: 1.9740 - accuracy: 0.3130 - val_loss: 1.9772 - val_accuracy: 0.3144\n",
      "Epoch 3/10\n",
      "14980/14980 [==============================] - 659s 44ms/step - loss: 1.9737 - accuracy: 0.3130 - val_loss: 1.9752 - val_accuracy: 0.3144\n",
      "Epoch 4/10\n",
      "14980/14980 [==============================] - 502s 34ms/step - loss: 1.9734 - accuracy: 0.3130 - val_loss: 1.9754 - val_accuracy: 0.3144\n",
      "Epoch 5/10\n",
      "14980/14980 [==============================] - 408s 27ms/step - loss: 1.9733 - accuracy: 0.3130 - val_loss: 1.9750 - val_accuracy: 0.3144\n",
      "Epoch 6/10\n",
      "14980/14980 [==============================] - 393s 26ms/step - loss: 1.9731 - accuracy: 0.3130 - val_loss: 1.9757 - val_accuracy: 0.3144\n",
      "Epoch 7/10\n",
      "14980/14980 [==============================] - 382s 25ms/step - loss: 1.9732 - accuracy: 0.3130 - val_loss: 1.9760 - val_accuracy: 0.3144\n",
      "Epoch 8/10\n",
      "14980/14980 [==============================] - 381s 25ms/step - loss: 1.9731 - accuracy: 0.3130 - val_loss: 1.9749 - val_accuracy: 0.3144\n",
      "Epoch 9/10\n",
      "14980/14980 [==============================] - 385s 26ms/step - loss: 1.9730 - accuracy: 0.3130 - val_loss: 1.9755 - val_accuracy: 0.3144\n",
      "Epoch 10/10\n",
      "14980/14980 [==============================] - 386s 26ms/step - loss: 1.9729 - accuracy: 0.3130 - val_loss: 1.9749 - val_accuracy: 0.3144\n",
      "2341/2341 [==============================] - 13s 6ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/CNN_topicModel.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/CNN_topicModel.ipynb#X15sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m y_pred_labels \u001b[39m=\u001b[39m label_binarizer\u001b[39m.\u001b[39minverse_transform(y_pred)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/CNN_topicModel.ipynb#X15sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Generate a classification report\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/CNN_topicModel.ipynb#X15sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m report \u001b[39m=\u001b[39m classification_report(y_test, y_pred)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeetusonthefetus/iiit/SMAI/project-boys/CNN/CNN_topicModel.ipynb#X15sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(report)\n",
      "File \u001b[0;32m~/iiit/SMAI/project-boys/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    215\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/iiit/SMAI/project-boys/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2545\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2410\u001b[0m \u001b[39m@validate_params\u001b[39m(\n\u001b[1;32m   2411\u001b[0m     {\n\u001b[1;32m   2412\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39marray-like\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msparse matrix\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2436\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2437\u001b[0m ):\n\u001b[1;32m   2438\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[1;32m   2439\u001b[0m \n\u001b[1;32m   2440\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2542\u001b[0m \u001b[39m    <BLANKLINE>\u001b[39;00m\n\u001b[1;32m   2543\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2545\u001b[0m     y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m   2547\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2548\u001b[0m         labels \u001b[39m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[0;32m~/iiit/SMAI/project-boys/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     90\u001b[0m     y_type \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_type) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mClassification metrics can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt handle a mix of \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m targets\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     95\u001b[0m             type_true, type_pred\n\u001b[1;32m     96\u001b[0m         )\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     99\u001b[0m \u001b[39m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    100\u001b[0m y_type \u001b[39m=\u001b[39m y_type\u001b[39m.\u001b[39mpop()\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping  # Import EarlyStopping\n",
    "\n",
    "# Convert y_train and y_test to one-hot encoded format\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_train_onehot = label_binarizer.fit_transform(y_train)\n",
    "y_test_onehot = label_binarizer.transform(y_test)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=topic_features.shape[1], output_dim=512, input_length=topic_features.shape[1]))\n",
    "model.add(Conv1D(512, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(len(label_binarizer.classes_), activation='softmax'))  # Number of classes for multi-class\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train_onehot, epochs=10, batch_size=16, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = label_binarizer.inverse_transform(y_pred)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
