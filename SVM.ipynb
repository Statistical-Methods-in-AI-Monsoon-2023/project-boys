{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load the data from combined_df.csv\n",
    "combined_df = pd.read_csv('./combined_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.drop('post', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74900, 347)\n"
     ]
    }
   ],
   "source": [
    "# randomly drop 80% of the data\n",
    "combined_df = combined_df.sample(frac=0.2, random_state=1)\n",
    "print(combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and clean text data (you can use regular expressions for more advanced cleaning)\n",
    "combined_df['post'] = combined_df['post'].apply(lambda x: x.lower())  # Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenization and removing punctuation\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "    \n",
    "    # Removing stop words and lemmatization\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [WordNetLemmatizer().lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return \" \".join(words)\n",
    "    \n",
    "combined_df['cleaned_posts'] = combined_df['post'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can load pre-trained word embeddings using libraries like gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train Word2Vec model on your preprocessed text data\n",
    "word2vec_model = Word2Vec(sentences=[text.split() for text in combined_df['cleaned_posts']], vector_size=100, window=5, min_count=1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to generate Word2Vec embeddings for a given text\n",
    "def get_word2vec_features(text, model):\n",
    "    words = text.split()\n",
    "    # Initialize an empty vector\n",
    "    feature_vector = np.zeros((model.vector_size,), dtype=\"float32\")\n",
    "    num_words = 0\n",
    "    # Iterate over each word in the text\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            num_words += 1\n",
    "            # Add the word's vector to the feature_vector\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    # Divide the result by the number of words to get the average\n",
    "    if num_words != 0:\n",
    "        feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector\n",
    "\n",
    "# Generate Word2Vec features for each post in the DataFrame\n",
    "combined_df['word2vec_features'] = combined_df['cleaned_posts'].apply(lambda post: get_word2vec_features(post, word2vec_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_arrays = np.array(list(combined_df['word2vec_features']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_word2vec = word2vec_arrays.shape\n",
    "\n",
    "# Create an empty array to accommodate the Word2Vec data\n",
    "X_combined = np.empty((shape_word2vec[0], shape_word2vec[1]))\n",
    "\n",
    "# Copy data from the original Word2Vec array to the combined array\n",
    "X_combined[:, :shape_word2vec[1]] = word2vec_arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = combined_df.drop(['subreddit'],  axis=1)  # Features\n",
    "y = combined_df['subreddit']  # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA on X_train\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pca , y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Encode the categorical target variable (y_train) using OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_encoded = encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Step 2: Flatten the encoded labels\n",
    "y_train_encoded = y_train_encoded.argmax(axis=1)\n",
    "y_test_encoded = encoder.transform(y_test.values.reshape(-1, 1)).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "\n",
    "    def __init__(self, C = 1.0):\n",
    "        # C = error term\n",
    "        self.C = C\n",
    "        self.w = 0\n",
    "        self.b = 0\n",
    "\n",
    "    # Hinge Loss Function / Calculation\n",
    "    def hingeloss(self, w, b, x, y):\n",
    "        # Regularizer term\n",
    "        reg = 0.5 * np.sum(w * w)\n",
    "\n",
    "        y = y.reshape(-1, 1)\n",
    "\n",
    "        # Calculate the optimization term for each data point\n",
    "        opt_term = y * (np.dot(w, x.T) + b)\n",
    "\n",
    "\n",
    "        # Calculate the loss for each data point\n",
    "        loss = reg + self.C * np.sum(np.maximum(0, 1 - opt_term))\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def fit(self, X, Y, batch_size=100, learning_rate=0.001, epochs=1000):\n",
    "        # The number of features in X\n",
    "        number_of_features = X.shape[1]\n",
    "\n",
    "        # The number of Samples in X\n",
    "        number_of_samples = X.shape[0]\n",
    "\n",
    "        c = self.C\n",
    "\n",
    "        # Creating ids from 0 to number_of_samples - 1\n",
    "        ids = np.arange(number_of_samples)\n",
    "\n",
    "        # Shuffling the samples randomly\n",
    "        np.random.shuffle(ids)\n",
    "\n",
    "        # creating an array of zeros\n",
    "        w = np.zeros((1, number_of_features))\n",
    "        b = 0\n",
    "        losses = []\n",
    "\n",
    "        # Gradient Descent \n",
    "        for i in range(epochs):\n",
    "            # Calculating the Hinge Loss\n",
    "            l = self.hingeloss(w, b, X, Y)\n",
    "\n",
    "            # Appending all losses \n",
    "            losses.append(l)\n",
    "            \n",
    "            # Starting from 0 to the number of samples with batch_size as interval\n",
    "            for batch_initial in range(0, number_of_samples, batch_size):\n",
    "                gradw = 0\n",
    "                gradb = 0\n",
    "\n",
    "                for j in range(batch_initial, batch_initial+ batch_size):\n",
    "                    if j < number_of_samples:\n",
    "                        x = ids[j]\n",
    "                        ti = Y[x] * (np.dot(w, X[x].T) + b)\n",
    "\n",
    "                        if (ti > 1).all():\n",
    "                            gradw += 0\n",
    "                            gradb += 0\n",
    "                        else:\n",
    "                            # Calculating the gradients\n",
    "\n",
    "                            #w.r.t w \n",
    "                            gradw += c * Y[x] * X[x]\n",
    "                            # w.r.t b\n",
    "                            gradb += c * Y[x]\n",
    "\n",
    "                # Updating weights and bias\n",
    "                w = w - learning_rate * w + learning_rate * gradw\n",
    "                b = b + learning_rate * gradb\n",
    "        \n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "        return self.w, self.b, losses\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        prediction = np.dot(X, self.w[0]) + self.b # w.x + b\n",
    "        return np.sign(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74900, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "[LibSVM]...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Train the SVM model with verbose output\n",
    "svm_model = SVC(kernel='linear', C=1e3, probability=True, verbose=1)\n",
    "print(\"here\") \n",
    "svm_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print classification report for detailed metrics\n",
    "print(classification_report(y_test_encoded, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
