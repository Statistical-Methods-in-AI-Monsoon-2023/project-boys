{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = './saved_BERT'\n",
    "\n",
    "# Load DistilBERT model and tokenizer\n",
    "tokenizer_BERT = transformers.DistilBertTokenizer.from_pretrained(save_directory)\n",
    "model_BERT = transformers.DistilBertForSequenceClassification.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./inferencing/data/BERT.csv')\n",
    "\n",
    "# Create test_texts and test_labels as lists\n",
    "\n",
    "test_texts = test_df['text'].tolist()\n",
    "test_labels = test_df['label'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying synonym replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yeetusonthefetus/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def replace_with_synonym(text, synonym_ratio=0.10):\n",
    "    tokens = tokenizer_BERT.tokenize(text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    num_words = len(words)\n",
    "    num_synonyms = int(num_words * synonym_ratio)\n",
    "    \n",
    "    indices = np.random.choice(num_words, size=num_synonyms, replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        word = words[idx]\n",
    "        synonyms = set()\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name())\n",
    "        if len(synonyms) > 0:\n",
    "            synonym = next(iter(synonyms))\n",
    "            tokens[idx] = synonym if tokens[idx] not in ['[CLS]', '[SEP]'] else tokens[idx]\n",
    "    \n",
    "    return tokenizer_BERT.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(text, mask_token='[MASK]', mask_ratio=0.60, words_to_mask=None):\n",
    "    if words_to_mask is None:\n",
    "        words_to_mask = ['depression', 'anxiety', 'suicidewatch', 'adhd', 'bpd', 'lonely',\n",
    "                         'autism', 'schizophrenia', 'ptsd', 'addiction', 'alcoholism',\n",
    "                         'depress', 'suicide', 'bipolar', 'addict', 'alchohol']\n",
    "    \n",
    "    words = text.split()\n",
    "    words_lower = [word.lower() for word in words]\n",
    "    mask_indices = []\n",
    "    \n",
    "    for idx, word in enumerate(words_lower):\n",
    "        if word in words_to_mask and np.random.rand() < mask_ratio:\n",
    "            mask_indices.append(idx)\n",
    "    \n",
    "    for idx in mask_indices:\n",
    "        words[idx] = mask_token\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts_syn = []\n",
    "\n",
    "for t in test_texts:\n",
    "    test_texts_syn.append(replace_with_synonym(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts_mask = []\n",
    "\n",
    "for t in test_texts:\n",
    "    test_texts_mask.append(apply_mask(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_fine_tuned = DistilBertTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "model_fine_tuned = DistilBertForSequenceClassification.from_pretrained(save_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(test_labels, test='base' ):\n",
    "    pred = []\n",
    "    if test == 'base':\n",
    "        for test_text in test_texts:\n",
    "            predict_input = tokenizer_BERT.encode_plus(\n",
    "                test_text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=300,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            output = model_BERT(**predict_input)[0]\n",
    "\n",
    "            prediction_value = torch.argmax(output, axis=1).item()\n",
    "            pred.append(prediction_value)\n",
    "    elif test == 'syn':\n",
    "        for test_text in test_texts_syn:\n",
    "            predict_input = tokenizer_fine_tuned.encode_plus(\n",
    "                test_text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=300,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            output = model_fine_tuned(**predict_input)[0]\n",
    "\n",
    "            prediction_value = torch.argmax(output, axis=1).item()\n",
    "            pred.append(prediction_value)\n",
    "    elif test == 'mask':\n",
    "        for test_text in test_texts_mask:\n",
    "            predict_input = tokenizer_fine_tuned.encode_plus(\n",
    "                test_text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=300,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            output = model_fine_tuned(**predict_input)[0]\n",
    "\n",
    "            prediction_value = torch.argmax(output, axis=1).item()\n",
    "            pred.append(prediction_value)\n",
    "            \n",
    "    total = len(pred)\n",
    "    correct = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == test_labels[i]:\n",
    "            correct += 1\n",
    "\n",
    "    return (float(correct)/float(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of stress testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base accuracy:  0.8628170894526035\n"
     ]
    }
   ],
   "source": [
    "base_acc = compute_accuracy(test_labels)\n",
    "\n",
    "print(\"Base accuracy: \", base_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym Accuracy:  0.8110814419225634\n"
     ]
    }
   ],
   "source": [
    "syn_acc =  compute_accuracy(test_labels , test='syn')\n",
    "\n",
    "print('Synonym Accuracy: ', syn_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask Accuracy:  0.815086782376502\n"
     ]
    }
   ],
   "source": [
    "mask_acc =  compute_accuracy(test_labels, test='mask')\n",
    "\n",
    "print('Mask Accuracy: ', mask_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
